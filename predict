'''import numpy as np
from keras.models import model_from_json
import operator
import time
import cv2
import sys, os
import streamlit as st
#from streamlit_webrtc import webrtc_streamer, VideoTransformerBase, RTCConfiguration, VideoProcessorBase, WebRtcMode
st.title("Sign Language Convertor")
run=st.checkbox('RUN')
inp=st.checkbox('Input')
frame_window=st.image([])

html_temp="""
<div style ="background-color:black;padding:13px"> 
</div> 
"""
st.markdown(html_temp, unsafe_allow_html = True) 
html_temp = """ 
    <div style ="background-color:white;padding:13px"> 
    </div> 
    """
st.markdown(html_temp, unsafe_allow_html = True)


json_file = open("model-bw.json", "r")
model_json = json_file.read()
json_file.close()
loaded_model = model_from_json(model_json)
loaded_model.load_weights("model-bw.h5")
print("Loaded model from disk")
l1=[]
cap = cv2.VideoCapture(0) 
categories = {0: 'my', 1: 'name', 2: 'hello', 3: 'THREE', 4: 'FOUR', 5: 'FIVE'}
while run:
        _, frame = cap.read()
        frame1=cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)
        frame = cv2.flip(frame, 1)
        frame1 = cv2.flip(frame1, 1)
        x1 = int(0.5*frame1.shape[1])
        y1 = 10
        x2 = frame1.shape[1]-10
        y2 = int(0.5*frame1.shape[1])
        cv2.rectangle(frame1, (x1-1, y1-1), (x2+1, y2+1), (255,0,0) ,1)
        roi = frame1[y1:y2, x1:x2]
        roi = cv2.resize(roi, (64, 64)) 
        roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        _, test_image = cv2.threshold(roi, 120, 255, cv2.THRESH_BINARY)
        cv2.imshow("test", test_image)
        frame_window.image(frame1)
        result = loaded_model.predict(test_image.reshape(1, 64, 64, 1))
        prediction = {'my': result[0][0], 
                  'name': result[0][1], 
                  'hello': result[0][2]
               }
        prediction = sorted(prediction.items(), key=operator.itemgetter(1), reverse=True)
        if b==0:
             a=prediction[0][0]
             st.write(prediction[0][0])
             l1.append(prediction[0][0])
             b+=1
        elif a!= prediction[0][0]: 
             st.write(prediction[0][0])
             l1.append(prediction[0][0])
             a=prediction[0][0]  
        cv2.putText(frame1, prediction[0][0], (10, 120), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)    
        cv2.imshow("Frame", frame1)
        interrupt = cv2.waitKey(10)
        if interrupt & 0xFF == 27:
             break     
else:
    st.write('Stopped')

cap.release()
cv2.destroyAllWindows()'''
import numpy as np
from keras.models import model_from_json
import operator
import time
import cv2
import sys, os
import streamlit as st
import pyttsx3
from gtts import gTTS
from playsound import playsound
def fun():
     f = open('speech.txt', 'r')
     file_text = f.read()
     language='en'
     out=gTTS(text='hello how are you',lang=language,slow=False)
     out.save("output.mp3")
     aud=open('output.mp3','rb')
     aud_bytes=aud.read()
     st.audio(aud_bytes,format='audio/ogg')
     run=False
     return run  

st.title("Sign Language Convertor")
run=st.checkbox('RUN')
inp=st.checkbox('Input')
frame_window=st.image([])

html_temp="""
<div style ="background-color:black;padding:13px"> 
</div> 
"""
st.markdown(html_temp, unsafe_allow_html = True) 
html_temp = """ 
    <div style ="background-color:white;padding:13px"> 
    </div> 
    """
st.markdown(html_temp, unsafe_allow_html = True)


json_file = open("model-bw.json", "r")
model_json = json_file.read()
json_file.close()
loaded_model = model_from_json(model_json)
loaded_model.load_weights("model-bw.h5")
print("Loaded model from disk")
l1=[]
cap = cv2.VideoCapture(0) 
categories = {0: 'my', 1: 'name', 2: 'hello', 3: 'THREE', 4: 'FOUR', 5: 'FIVE'}
b=0
s=' '
while run:
     # with open('speech.txt', 'w') as f:
        su=open('speech.txt','a')
        _, frame = cap.read()
        frame1=cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)
        #frame = cv2.flip(frame, 1)
        frame1 = cv2.flip(frame1, 1)
        x1 = int(0.5*frame1.shape[1])
        y1 = 10
        x2 = frame1.shape[1]-10
        y2 = int(0.5*frame1.shape[1])
        cv2.rectangle(frame1, (x1-1, y1-1), (x2+1, y2+1), (255,0,0) ,1)
        roi = frame1[y1:y2, x1:x2]
        roi = cv2.resize(roi, (64, 64)) 
        roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        _, test_image = cv2.threshold(roi, 120, 255, cv2.THRESH_BINARY)
        frame_window.image(frame1)
        result = loaded_model.predict(test_image.reshape(1, 64, 64, 1))
        prediction = {'my': result[0][0], 
                  'name': result[0][1], 
                  'hello': result[0][2]
               }
        prediction = sorted(prediction.items(), key=operator.itemgetter(1), reverse=True)
        if b==0:
             a=prediction[0][0]
             #st.write(prediction[0][0])
             l1.append(prediction[0][0])
             b+=1
        elif a!= prediction[0][0]: 
             #st.write(prediction[0][0])
             l1.append(prediction[0][0])
             a=prediction[0][0]
             s=s+a
             su.write(s)
           
        cv2.putText(frame1, prediction[0][0], (10, 120), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)    
        cv2.imshow("Frame", frame1)
        interrupt = cv2.waitKey(10)
        if interrupt & 0xFF == 27:
             break
     #    with open('speech.txt', 'w') as f:
     #         for line in l1:
     #              f.write(line)
     #         f.write('\n')
                                 
else: 
     st.write('Stopped')    
if inp:
     fun()
   

cap.release()
cv2.destroyAllWindows()
